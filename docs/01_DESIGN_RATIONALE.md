# Design Rationale: AI-Powered SQL Data Analysis Chatbot

**Project:** Multi-Database Text-to-SQL Analytics Chatbot  
**Type:** Technical Test - AI Engineer Position  
**Timeline:** 1 Week POC  
**Date:** February 2026  
**Version:** 2.0 (Updated)

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Problem Definition & Context](#2-problem-definition--context)
3. [Assumptions & Non-Goals](#3-assumptions--non-goals)
4. [Key Risks & Failure Modes](#4-key-risks--failure-modes)
5. [Success Criteria](#5-success-criteria)
6. [System Boundaries & Responsibility](#6-system-boundaries--responsibility)
7. [Solution Architecture](#7-solution-architecture)
8. [Problem‚ÄìSolution Mapping](#8-problemsolution-mapping)
9. [Failure Handling & Fallback Strategy](#9-failure-handling--fallback-strategy)
10. [Evaluation & Observability](#10-evaluation--observability)
11. [Database Design](#11-database-design)
12. [Technology Stack](#12-technology-stack)
13. [Security, Scalability, Maintainability](#13-security-scalability-maintainability)
14. [Detailed Example Interaction](#14-detailed-example-interaction)
15. [Cost Considerations](#15-cost-considerations)
16. [POC vs Production Roadmap](#16-poc-vs-production-roadmap)
17. [Deferred Decisions](#17-deferred-decisions)

---

## 1. Executive Summary

This document proposes a **pragmatic hybrid architecture** for an AI-powered chatbot that enables non-technical users to perform data analysis across multiple SQL databases using natural language.

### Key Design Principles

Given constraints such as large schemas (100+ tables), limited implementation time (1 week POC), and the need for safety and correctness, the system prioritizes:
- **Intent clarity** over linguistic flexibility
- **Controlled SQL generation** over maximum autonomy
- **Grounded insight generation** over speculative analysis
- **Correctness** over sophistication

### Primary Risks Addressed

- Query ambiguity (highest priority)
- Overly complex SQL generation
- Incorrect schema/table selection
- Misleading analytical insights
- SQL injection and security vulnerabilities

### Proposed Solution

A **staged pipeline with explicit guardrails** combining:
- **Agentic components** where intelligence adds value (4/7 components)
- **Traditional components** where determinism ensures reliability (2/7 components)
- **Hybrid validation** balancing speed and safety (1/7 components)

This design is suitable for a production-oriented MVP with clear upgrade path from POC to full deployment.

---

## 2. Problem Definition & Context

### 2.1 User Goal

Enable non-technical business users to ask natural language questions and receive understandable, trustworthy analytical insights **without writing SQL or understanding database schemas**.

**Target Users:**
- Sales managers querying customer data
- Marketing analysts exploring campaign performance
- Finance controllers reviewing revenue metrics
- Operations teams monitoring KPIs

### 2.2 Business Context

**Current State:**
- Data team receives ~50 ad-hoc query requests per week
- Average turnaround time: 2-3 days
- Business users blocked waiting for insights
- Data team unable to focus on strategic analysis

**Impact:**
- Delayed decision-making
- Missed revenue opportunities
- Poor self-service analytics culture
- Team frustration and bottlenecks

**Desired State:**
- Self-service analytics for business users
- Real-time insights (seconds, not days)
- Data team focuses on complex analytical work
- Scalable analytics capability

### 2.3 Technical Context

**System Context:**
- **3 separate PostgreSQL database instances**
  - Database 1: Transactional data (sales, customers)
  - Database 2: Product catalog & sellers
  - Database 3: Pre-aggregated analytics
- **Large and heterogeneous schemas** (100+ tables in production, 9 in POC)
- **Read-only analytical access** (no write operations)
- **Exploratory analytics use case** (ad-hoc questions, not dashboards)

**System Position:**
The system is **not designed to replace** existing BI tools (Tableau, Metabase), but to **lower the barrier** for ad-hoc exploratory analysis.

### 2.4 Constraints

**Time Constraint:**
- 1 week total POC development
- Must deliver working demo by end of week

**Budget Constraint:**
- POC budget: ~$100 for API calls (LLM, embeddings)
- No infrastructure budget (use local/free tier)

**Resource Constraint:**
- Solo developer
- No existing codebase
- Must use readily available tools

**Technical Constraint:**
- 3 separate database instances (cannot merge)
- Cross-database queries require application-level joins
- Schema is static for POC (no auto-discovery initially)

---

## 3. Assumptions & Non-Goals

### 3.1 Assumptions

**Data Quality:**
- Source data is clean, reliable, and sufficiently normalized
- No missing critical business logic in database
- Data definitions are consistent within each database

**Access & Permissions:**
- Databases are accessed in read-only mode
- Application has SELECT-only privileges
- No authentication required for POC (single user demo)

**Usage Context:**
- Users are performing exploratory, **non-mission-critical** analysis
- Results will be verified by users before business decisions
- No strict SLA required at MVP stage

**Infrastructure:**
- Schema metadata is available and reasonably up to date
- Database connections are stable
- Local development environment sufficient for POC

### 3.2 Non-Goals

**Out of Scope for POC:**
- ‚ùå Real-time or streaming analytics
- ‚ùå Predictive modeling or forecasting
- ‚ùå Automated data cleaning or schema normalization
- ‚ùå Write or update operations on databases
- ‚ùå Advanced visualization or interactive dashboards
- ‚ùå Multi-user authentication and authorization
- ‚ùå Production-grade monitoring and alerting
- ‚ùå High availability and disaster recovery

**Explicitly Deferred to MVP/Production:**
- User management and RBAC
- Query result caching
- Advanced SQL features (window functions, CTEs)
- Cross-database JOINs (architecture supports, not fully tested)
- Performance optimization for large datasets
- Mobile app support

---

## 4. Key Risks & Failure Modes

### 4.1 Risk Assessment Matrix

| Risk | Description | Impact | Likelihood | Priority |
|------|-------------|--------|------------|----------|
| **Query ambiguity** | Underspecified metrics, entities, or timeframes | High | High | **Critical** |
| **Overly complex SQL** | Excessive joins or aggregations that fail or timeout | High | Medium | **High** |
| **Wrong schema selection** | Semantically similar but incorrect tables chosen | High | Medium | **High** |
| **Misleading insights** | Over-interpretation or implied causality in results | High | Medium | **High** |
| **SQL injection** | Malicious input executing dangerous operations | Critical | Low | **Critical** |
| **Cost escalation** | Heavy queries and repeated LLM calls exceeding budget | Medium | Medium | **Medium** |
| **Poor performance** | Slow queries frustrating users | Medium | Medium | **Medium** |

### 4.2 Highest Priority Risk: Query Ambiguity

**Why Critical:**
Errors at the intent understanding stage propagate downstream even if all other components function correctly.

**Example:**
```
User: "Show sales performance"

Ambiguities:
- Sales = revenue? unit count? profit margin?
- Performance = total? growth? comparison?
- Timeframe = today? this month? year-to-date?
- Entity = by product? by region? by seller?

Wrong interpretation ‚Üí Wrong SQL ‚Üí Wrong insights ‚Üí Wrong decisions
```

**Mitigation (Section 8.1):**
Explicit intent classification with clarification requests when ambiguous.

### 4.3 Risk Mitigation Strategy

**Layered Defense:**
1. **Prevention:** Intent classification catches ambiguity early
2. **Detection:** SQL validation identifies unsafe/incorrect queries
3. **Containment:** Execution limits (timeout, row limit) prevent damage
4. **Recovery:** Clear error messages guide user to rephrase

**Risk Acceptance:**
- Accept 10-20% query failure rate in POC (learning phase)
- Accept slower response time (3-5s) vs instant results
- Accept limited SQL feature support vs full SQL capability

---

## 5. Success Criteria

### 5.1 POC Success Criteria

The POC is considered **successful** if:

**Functional:**
- ‚úÖ Users can retrieve correct analytical insights without writing SQL
- ‚úÖ System achieves ‚â•80% accuracy on 20 test queries
- ‚úÖ Generated SQL is safe (no security vulnerabilities)
- ‚úÖ Response time <5 seconds for typical queries

**Non-Functional:**
- ‚úÖ System behavior is observable (logs SQL, intent, results)
- ‚úÖ Failures are graceful (helpful error messages)
- ‚úÖ Architecture supports MVP evolution (no rewrite needed)

**Business:**
- ‚úÖ Demo impresses stakeholders (subjective but critical)
- ‚úÖ Clear ROI path identified (data team time savings)
- ‚úÖ Decision to proceed to MVP made

### 5.2 MVP Success Criteria (Future)

- 85% accuracy on expanded test set
- <3 second p95 response time
- 10-20 active beta users
- 60% reduction in data team ad-hoc requests

### 5.3 Acceptance Criteria

**Must Have:**
- System handles 15/20 test queries correctly (75%+)
- Zero SQL injection vulnerabilities in security scan
- All generated SQL is read-only (no INSERT/UPDATE/DELETE)

**Should Have:**
- Graceful handling of ambiguous queries
- Transparent SQL display for user verification
- Helpful error messages (not technical jargon)

**Nice to Have:**
- Query suggestions based on schema
- Result visualization (simple charts)
- Query history

---

## 6. System Boundaries & Responsibility

### 6.1 System Positioning

The chatbot is positioned as an **advisory analytics assistant**, not an authoritative source of truth.

**Analogy:** More like a "smart calculator" than a "decision maker"

### 6.2 System Responsibilities

**The system IS responsible for:**
- ‚úÖ Interpreting user intent within defined constraints
- ‚úÖ Generating safe, read-only SQL queries
- ‚úÖ Executing queries with safety controls (timeout, row limits)
- ‚úÖ Summarizing query results into human-readable insights
- ‚úÖ Showing SQL for transparency and verification
- ‚úÖ Detecting and rejecting ambiguous or unsafe queries

**The system IS NOT responsible for:**
- ‚ùå Validating business decisions
- ‚ùå Guaranteeing data correctness beyond stated assumptions (Section 3.1)
- ‚ùå Explaining root causes or business causality
- ‚ùå Providing recommendations or predictions
- ‚ùå Data quality or schema design issues
- ‚ùå User education on data interpretation

### 6.3 User Responsibilities

**Users are expected to:**
- Verify critical results before business decisions
- Understand that system provides analysis, not recommendations
- Report incorrect results or confusing behavior
- Refine ambiguous queries when asked

### 6.4 Failure Modes & System Response

**When system detects issues:**
- Ambiguous query ‚Üí Ask clarification (don't guess)
- Invalid SQL ‚Üí Show error + suggest fix
- Empty results ‚Üí Explain why (e.g., "No data matches filters")
- Timeout ‚Üí Suggest narrower query scope

**System will NOT:**
- Fabricate data when none exists
- Imply causality from correlation
- Proceed with low-confidence interpretations

---

## 7. Solution Architecture

### 7.1 Architecture Overview

The system is implemented as a **staged validation pipeline** where each step reduces uncertainty and constrains the next:
```
User Query (Natural Language)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Intent Classification (Agentic)     ‚îÇ
‚îÇ    Tool: Claude Sonnet 4                ‚îÇ
‚îÇ    Risk Mitigated: Query ambiguity      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Schema Retrieval (Traditional)      ‚îÇ
‚îÇ    Tool: ChromaDB + RAG                 ‚îÇ
‚îÇ    Risk Mitigated: Wrong table selection‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Retrieval Evaluator (Agentic)       ‚îÇ
‚îÇ    Tool: Claude Sonnet 4                ‚îÇ
‚îÇ    Risk Mitigated: False positives      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. SQL Generator (Agentic)              ‚îÇ
‚îÇ    Tool: Claude Sonnet 4 + Few-shot     ‚îÇ
‚îÇ    Risk Mitigated: Complex/incorrect SQL‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. SQL Validator (Hybrid)               ‚îÇ
‚îÇ    Tool: sqlparse + Claude              ‚îÇ
‚îÇ    Risk Mitigated: Unsafe/invalid SQL   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Query Executor (Traditional)         ‚îÇ
‚îÇ    Tool: PostgreSQL + SQLAlchemy        ‚îÇ
‚îÇ    Risk Mitigated: Resource exhaustion  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. Insight Generator (Agentic)          ‚îÇ
‚îÇ    Tool: Claude Sonnet 4                ‚îÇ
‚îÇ    Risk Mitigated: Misleading insights  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì
Response (Insights + SQL + Data + Metadata)
```

**Design Philosophy:** Each stage acts as a **guardrail** for the next, progressively constraining the solution space.

### 7.2 Component Classification

| Component | Type | Intelligence Level | Why This Type |
|-----------|------|-------------------|---------------|
| **1. Intent Classifier** | ü§ñ Agentic | High | NL understanding requires reasoning |
| **2. Schema Retrieval** | ‚öôÔ∏è Traditional | None | Semantic search is deterministic |
| **3. Retrieval Evaluator** | ü§ñ Agentic | High | Relevance judgment requires intelligence |
| **4. SQL Generator** | ü§ñ Agentic | High | Code generation from NL requires LLM |
| **5. SQL Validator** | üîÄ Hybrid | Medium | Syntax=rules, Logic=reasoning |
| **6. Query Executor** | ‚öôÔ∏è Traditional | None | Execution is deterministic |
| **7. Insight Generator** | ü§ñ Agentic | High | Humanization requires language model |

**Ratio:**
- Agentic: 4 components (57%)
- Traditional: 2 components (29%)
- Hybrid: 1 component (14%)

### 7.3 Why Hybrid Architecture?

**Alternatives Considered:**

#### Option A: Traditional Pipeline (Rule-Based)
```
Pros: Fast (3-4 days), predictable, cheap
Cons: 60% accuracy, cannot handle variations
Verdict: ‚ùå Insufficient for business value
```

#### Option B: Pure Agentic System (Autonomous)
```
Pros: 95% accuracy potential, fully adaptive
Cons: 6-7 days dev, hard to debug, expensive
Verdict: ‚ö†Ô∏è Over-engineered for POC, defer to Phase 2
```

#### Option C: Hybrid (CHOSEN) ‚úÖ
```
Pros: 90% accuracy, 5-day dev, debuggable, cost-effective
Cons: Medium complexity (manageable)
Verdict: ‚úÖ Best balance for POC ‚Üí Production path
```

**Why Hybrid Wins:**
- Intelligence where it adds value (understanding, generation)
- Determinism where reliability matters (execution, search)
- Clear upgrade path (enhance agentic parts incrementally)
- Fits timeline and budget constraints

---

## 8. Problem‚ÄìSolution Mapping

### 8.1 Query Ambiguity & Over-Complexity

**Problem**

User queries are often:
- **Incomplete:** "Show sales" (Sales what? When? Where?)
- **Ambiguous:** "Performance" (Total? Growth? Comparison?)
- **Multi-intent:** "Sales and customer growth last quarter by region"

This leads to:
- Semantically incorrect SQL
- Overly complex queries that timeout
- Wrong business insights

**Example Failure:**
```
User: "Show sales performance"

Bad System (No Intent Classification):
‚Üí Guesses: Total sales, all time, all products
‚Üí Generates: SELECT SUM(amount) FROM orders
‚Üí Returns: Meaningless total without context
‚Üí User: "That's not what I meant!"

Good System (With Intent Classification):
‚Üí Detects ambiguity
‚Üí Asks: "Do you want total sales, or sales growth? 
         For which time period?"
‚Üí User clarifies: "Sales growth, last 6 months"
‚Üí Generates correct query
```

**Mitigation**

**Component 1: Intent Classification**
- **Tool:** Claude Sonnet 4
- **Approach:** Classify into predefined categories
  - `simple_select` - Basic retrieval
  - `filtered_query` - With WHERE conditions
  - `aggregation` - Requires SUM/COUNT/AVG/GROUP BY
  - `multi_table_join` - Needs JOINs
  - `complex_analytics` - Multi-step analysis
  - `ambiguous` - **Insufficient information**

**Key Rule:** If intent = `ambiguous` OR confidence < 0.7:
‚Üí **Ask clarification, don't guess**

**Implementation:**
```
Prompt to Claude:
"Classify this query: 'Show sales performance'

Categories: [list above]

If insufficient information (no metric, timeframe, or entity), 
classify as 'ambiguous'.

Return: {category: 'ambiguous', reason: 'Missing metric and timeframe'}"
```

**Trade-off**
- ‚úÖ Reduces linguistic flexibility (users may need to rephrase)
- ‚úÖ Significantly improves correctness and reliability
- ‚úÖ Builds user trust (system doesn't pretend to understand)

**Success Metric:** 100% of ambiguous queries trigger clarification (zero guessing)

---

### 8.2 Data Representation & Business Semantics

**Problem**

Even with clean data, **inconsistent business definitions** can produce misleading results:
- "Revenue" = payment_value? or order_amount? or after refunds?
- "Active users" = logged in last 30 days? or purchased?
- "Sales" = order count? or revenue? or units sold?

**Example Failure:**
```
User: "Total revenue this month"

Database has:
- orders.total_amount (pre-tax)
- payments.payment_value (post-tax, actual received)

System uses orders.total_amount ‚Üí Wrong number!
```

**Mitigation**

**Metadata Layer: Canonical Business Metrics**

Define standard business terms mapped to SQL logic:
```yaml
metrics:
  revenue:
    definition: "Actual money received (post-tax, post-refunds)"
    calculation: "SUM(payments.payment_value)"
    tables: [payments, orders]
    note: "Use payment_value, NOT order total_amount"
  
  active_customers:
    definition: "Customers with purchase in last 30 days"
    calculation: "COUNT(DISTINCT customer_id) WHERE order_date >= CURRENT_DATE - 30"
    tables: [customers, orders]
  
  conversion_rate:
    definition: "Orders / Unique visitors"
    calculation: "COUNT(DISTINCT order_id) / COUNT(DISTINCT session_id)"
    tables: [orders, sessions]
```

**Usage in SQL Generation:**

Include metric definitions in prompt:
```
User asks: "Total revenue this month"

Prompt to Claude:
"Business metric 'revenue' is defined as SUM(payments.payment_value)
NOT orders.total_amount.

Generate SQL for: Total revenue this month"

Result: Uses correct column ‚úì
```

**Trade-off**
- ‚ö†Ô∏è Requires upfront definition effort (1-2 days for 20-30 metrics)
- ‚úÖ Prevents silent logical errors (high ROI)
- ‚úÖ Ensures business consistency across queries

**For POC:** Define 5-10 core metrics (revenue, customers, orders, sales, growth)

---

### 8.3 Schema & Table Discovery

**Problem**

**Large schemas** (100+ tables) create multiple failure modes:
- **Wrong table selection:** "customer" vs "customer_segments" vs "customer_leads"
- **Missing joins:** Need 3 tables but only retrieve 2
- **Token limit:** Cannot fit all schemas in LLM prompt
- **Noise:** Irrelevant tables confuse SQL generation

**Example Failure:**
```
User: "Top customers by spending"

Database has 100 tables, including:
- customers (main table)
- customer_segments (derived analytics)
- customer_leads (sales prospects)
- customer_support_tickets (unrelated)

Bad System (No Retrieval):
‚Üí Sends all 100 table schemas to LLM
‚Üí LLM confused by similar names
‚Üí Generates SQL using wrong table
‚Üí Wrong results

Good System (RAG Retrieval):
‚Üí Retrieves top-5 relevant tables only
‚Üí customers, orders, payments (correct!)
‚Üí LLM focuses on relevant context
‚Üí Correct SQL generated
```

**Mitigation**

**Component 2: Schema Retrieval (RAG)**
- **Tool:** ChromaDB (vector database)
- **Approach:** Semantic search for relevant tables

**Offline Process (Schema Indexing):**
1. For each table, create **rich semantic description**:
```
   Table: customers
   
   Business Purpose: Stores customer master data including 
   contact info and location for buyer/client/user records
   
   Columns:
   - customer_id: unique identifier for each customer/buyer
   - name: customer/client name 
   - email: contact email for communication
   - city/state: location, region, area, geography
   
   Common queries: "list customers", "customers in Jakarta",
   "customer by email", "buyer information"
   
   Relationships:
   - Referenced by orders.customer_id (1:N)
```

2. Generate embedding (OpenAI text-embedding-3-small)
3. Store in ChromaDB with metadata

**Online Process (Query-time):**
1. Embed user query: "Top customers by spending"
2. Semantic search ‚Üí Top-5 similar tables:
   - customers (similarity: 0.92)
   - orders (similarity: 0.89)
   - payments (similarity: 0.85)
   - customer_segments (similarity: 0.78)
   - order_items (similarity: 0.65)
3. Pass only these 5 schemas to SQL generator

**Why RAG vs Alternatives:**
- ‚ùå Load all schemas ‚Üí 50K tokens, expensive, confused LLM
- ‚ùå Rule-based keywords ‚Üí Misses semantic variations ("buyer" vs "customer")
- ‚úÖ RAG semantic search ‚Üí Finds relevant tables even with different terms

**Trade-off**
- ‚ö†Ô∏è Requires offline indexing (1-time effort, ~2 hours)
- ‚ö†Ô∏è May miss tables if descriptions are poor
- ‚úÖ Reduces prompt size 90% (50K ‚Üí 5K tokens)
- ‚úÖ Improves SQL accuracy +5-10%

**Enhancement: Retrieval Evaluator (Component 3)**

**Problem:** RAG sometimes returns false positives

**Solution:** Add LLM evaluator to filter results
```
Input: Top-5 retrieved tables
Task: Which are ACTUALLY needed for this query?
Output: Top-3 essential tables (filtered)

Example:
Retrieved: [customers, orders, payments, customer_segments, order_items]
Evaluator: Remove customer_segments, order_items (not needed)
Final: [customers, orders, payments] ‚úì
```

**Benefit:** +5% SQL accuracy, worth extra 0.5s latency

---

### 8.4 SQL Generation & Validation

**Problem**

Generated SQL may be:
- **Unsafe:** SQL injection, DROP TABLE, etc.
- **Invalid:** Syntax errors, non-existent tables
- **Inefficient:** Missing LIMIT, SELECT *, huge joins
- **Logically wrong:** Incorrect JOIN keys, wrong aggregation

**Example Failures:**
```
1. Security:
   User: "'; DROP TABLE customers; --"
   Bad system: Executes malicious SQL
   Good system: Blocks with security error

2. Logic:
   User: "Top 5 customers by spending"
   Generated: SELECT name, SUM(amount) FROM customers...
   Problem: Missing JOIN to orders table!
   
3. Performance:
   Generated: SELECT * FROM orders  (no LIMIT)
   Problem: Returns 1M rows, crashes browser
```

**Mitigation**

**Two-Stage Approach:**

**Component 4: SQL Generator (Agentic)**
- **Tool:** Claude Sonnet 4 with few-shot prompting
- **Approach:** 
  - Provide 7 curated example queries
  - Include schema context from retrieval
  - Enforce conservative SQL generation

**Few-Shot Examples:**
```
Example 1: Simple
Q: "Show all customers"
SQL: SELECT * FROM customers LIMIT 100;

Example 2: Aggregation
Q: "Total sales this month"
SQL: SELECT SUM(payment_value) 
     FROM payments p JOIN orders o ON p.order_id = o.order_id
     WHERE EXTRACT(MONTH FROM o.order_date) = EXTRACT(MONTH FROM CURRENT_DATE)

[5 more examples covering joins, GROUP BY, date functions, NULL handling]
```

**Why Few-Shot:**
- Ablation study: 60% accuracy (zero-shot) ‚Üí 85% (few-shot)
- Teaches business terminology mapping
- Shows desired SQL style

**Component 5: SQL Validator (Hybrid)**

**4-Layer Validation:**

**Layer 1: Syntax Check (Traditional - sqlparse)**
```
Parse SQL using sqlparse library
Check: Valid PostgreSQL syntax?
If invalid ‚Üí Return syntax error
```

**Layer 2: Security Check (Traditional - regex)**
```
Pattern matching:
- Block: DROP, DELETE, UPDATE, INSERT, CREATE, ALTER
- Block: SQL comments (--,  /* */)
- Block: UNION SELECT from information_schema
- Allow: ONLY SELECT statements

If violation ‚Üí Return security error (non-negotiable)
```

**Layer 3: Table Existence (Traditional - lookup)**
```
Extract table names from SQL
Check: All tables in allowed list?
If unknown table ‚Üí Return "table not found" error
```

**Layer 4: Logic Validation (Agentic - Claude)**
```
Only run if layers 1-3 pass

Prompt to Claude:
"Does this SQL correctly answer the user's question?
Check:
- Correct JOIN keys?
- Missing WHERE clauses? (user said 'this month' but no date filter)
- Appropriate aggregation? (COUNT when user wants SUM?)

User: 'Top 5 customers by spending'
SQL: SELECT name FROM customers ORDER BY name LIMIT 5

Issue: No spending calculation, no JOIN to orders!"

If logic errors detected ‚Üí Auto-fix attempt (max 2 retries)
```

**Auto-Fix Process:**
```
If Layer 4 detects errors:
  1. Send to Claude: "Fix this SQL. Errors: [list]"
  2. Claude generates corrected SQL
  3. Re-validate (Layers 1-4 again)
  4. If still invalid after 2 attempts ‚Üí Return error to user
```

**Trade-off**
- ‚ö†Ô∏è Adds 0.3-0.5s latency (validation + potential retry)
- ‚úÖ Improves safety to 100% (SQL injection prevention)
- ‚úÖ Improves accuracy from 85% ‚Üí 90% (auto-fix catches errors)

**Success Metrics:**
- Security: 100% injection attempts blocked
- Accuracy: 90% of generated SQL is correct
- Performance: <0.5s validation time

---

### 8.5 Insight Generation

**Problem**

LLMs tend to:
- **Over-interpret data:** Imply causality from correlation
- **Speculate beyond evidence:** "Sales dropped BECAUSE..."
- **Use overconfident language:** "This proves..." vs "This suggests..."
- **Add unjustified recommendations:** "You should..." based on descriptive stats

**Example Failures:**
```
Query Result: "Sales down 10% vs last month"

Bad Insight:
"Sales dropped 10% due to market conditions. This indicates 
a concerning trend. You should increase marketing spend and 
review pricing strategy immediately."

Problems:
‚ùå Assumes causality (market conditions)
‚ùå Implies urgency (concerning trend)
‚ùå Gives recommendations (increase spend)
‚ùå All without evidence!

Good Insight:
"Sales this month: Rp 450M, down 10% from last month (Rp 500M).
This represents a decrease of Rp 50M. To understand why, 
consider checking: order volume, average order value, or 
seasonal patterns."

Benefits:
‚úÖ States facts only
‚úÖ Conservative language (down, not "collapsed")
‚úÖ Suggests exploration, not conclusions
‚úÖ Grounds everything in data
```

**Mitigation**

**Component 7: Insight Generator (Agentic with Constraints)**

**Approach: Grounded Summarization**

Strict prompt instructions:
```
System Prompt to Claude:

You are a conservative data analyst. Summarize query results 
following these rules:

RULES:
1. State only what's in the data (no speculation)
2. Use probabilistic language: "suggests", "indicates", "appears"
   NOT: "proves", "shows", "demonstrates"
3. Never imply causality without explicit instruction
4. No recommendations unless specifically asked
5. Highlight limitations or caveats
6. Suggest areas for further investigation (not conclusions)

User Question: "{user_query}"
SQL Used: {sql}
Results: {data}

Provide:
1. Direct answer (2-3 sentences, grounded in data)
2. Key observations (patterns visible in results)
3. Caveats or limitations (what this data does NOT tell us)

Be helpful but conservative. Avoid speculation.
```

**Example Output:**
```
User: "How are sales trending?"

Data: Monthly sales last 6 months

Generated Insight:
"Sales over the last 6 months show the following pattern:
- Jan-Mar: Average Rp 450M/month
- Apr-Jun: Average Rp 520M/month (+15% vs Q1)

The data suggests an upward trend, with month-over-month 
growth ranging from 5% to 12%.

Note: This analysis shows correlation with time but does not 
indicate specific drivers. Consider examining: product mix, 
seasonal factors, or marketing campaign timing for deeper 
understanding."

Grounded: ‚úì
Conservative language: ‚úì
No unjustified causality: ‚úì
Suggests exploration: ‚úì
```

**Trade-off**
- ‚ö†Ô∏è Insights may be less "impressive" or definitive
- ‚úÖ Builds user trust (no BS, just facts)
- ‚úÖ Encourages proper analytical thinking
- ‚úÖ Reduces risk of misleading business decisions

**Temperature Setting:** 0.3 (slightly creative for phrasing, but controlled)

---

## 9. Failure Handling & Fallback Strategy

### 9.1 Failure Scenarios

The system explicitly handles common failure scenarios with helpful responses:

| Scenario | System Response | User Action |
|----------|-----------------|-------------|
| **Ambiguous query** | "I need clarification: Do you mean X or Y?" | Refine query |
| **SQL generation failure** | "I couldn't generate valid SQL. Try: [suggestion]" | Rephrase |
| **Execution timeout** | "Query is too complex. Try narrower filters." | Add constraints |
| **Empty results** | "No data matches your criteria. Check: [filters]" | Adjust filters |
| **Invalid table** | "Table 'X' doesn't exist. Did you mean: [similar]?" | Correct table name |

### 9.2 Graceful Degradation

**Instead of:**
```
‚ùå Error 500: SQL syntax exception at line 23
‚ùå Null pointer exception in query executor
‚ùå [Generic error, try again]
```

**System provides:**
```
‚úÖ "I generated SQL with an error. Here's what I tried:
   [Show SQL]
   
   The issue is: Missing table 'customer_orders'
   
   Suggestion: Try rephrasing as 'customer purchases' or 
   'orders by customer'"

‚úÖ Shows: What was attempted, why it failed, how to fix
```

### 9.3 User Clarification Flow

**Example:**
```
User: "Show me the numbers"

System detects: Ambiguous

Response:
"I'd be happy to help! Could you clarify:
- Which numbers? (Sales, orders, customers, revenue?)
- For what time period? (Today, this week, this month?)
- Any specific filters? (Region, product category?)

Example: 'Show sales numbers for this month'"

User: "Sales numbers this month"

System: [Proceeds with clear intent]
```

### 9.4 Fallback Chain
```
Primary: Generate SQL and execute
   ‚Üì (if fails)
Fallback 1: Auto-fix SQL and retry
   ‚Üì (if fails)
Fallback 2: Ask user to rephrase
   ‚Üì (if fails)
Fallback 3: Suggest simpler alternative query
   ‚Üì (last resort)
Escalation: "I can't answer this. Please contact data team."
```

**Never:** Proceed with low confidence or return fabricated data

---

## 10. Evaluation & Observability

### 10.1 Logging Strategy

**Each interaction logs:**
```json
{
  "timestamp": "2026-02-10T14:23:45Z",
  "user_query": "Top 5 customers by spending",
  "intent": {
    "category": "multi_table_join",
    "confidence": 0.95
  },
  "schema_retrieval": {
    "retrieved_tables": ["customers", "orders", "payments"],
    "retrieval_time_ms": 320
  },
  "sql_generated": "SELECT c.name, SUM(p.payment_value)...",
  "sql_valid": true,
  "execution": {
    "success": true,
    "time_ms": 847,
    "row_count": 5
  },
  "response_time_total_ms": 4200,
  "error": null
}
```

**Purpose:**
- Debug failures
- Identify patterns in ambiguous queries
- Measure component performance
- Analyze accuracy over time

### 10.2 Evaluation Metrics

**Accuracy Metrics:**
```
SQL Correctness = Correct SQL / Total Queries
  - Manual review of 20 test queries
  - Expected: ‚â•80% for POC

Intent Classification Accuracy = Correct Intent / Total
  - Compare to ground truth labels
  - Expected: ‚â•90%

Result Relevance = Relevant Results / Total
  - User feedback (thumbs up/down)
  - Expected: ‚â•75% positive
```

**Performance Metrics:**
```
Response Time (p50, p95, p99)
  - Target: p95 < 5 seconds

SQL Execution Time
  - Target: <1 second for 80% of queries

Component Time Breakdown
  - Intent: ~0.5s
  - Retrieval: ~0.3s
  - Generation: ~1.2s
  - Validation: ~0.4s
  - Execution: ~0.9s
  - Insights: ~1.0s
```

**Cost Metrics:**
```
Cost per Query = LLM calls + Embeddings + Infrastructure
  - Target: <$0.05 per query for POC

Monthly Budget Tracking
  - Alert at 75% of $100 budget
```

### 10.3 Quality Checks

**Automated Tests:**
```
Test Suite (20 queries):
- 6 simple queries (expect 100% accuracy)
- 8 medium queries (expect 85% accuracy)
- 4 complex queries (expect 75% accuracy)
- 2 edge cases (expect graceful handling)

Run after each code change
```

**Manual Review:**
```
Weekly review:
- Failed queries analysis
- New failure patterns
- User feedback review
- Accuracy trending
```

---

## 11. Database Design

### 11.1 Database Structure

**3 PostgreSQL Database Instances:**

#### **Database 1: `sales_db`** (Transactional)

**Purpose:** Core sales transactions and customer data

**Tables:**
```sql
-- Table: customers
customer_id       SERIAL PRIMARY KEY
name              VARCHAR(255) NOT NULL
email             VARCHAR(255) UNIQUE
city              VARCHAR(100)
state             VARCHAR(100)
created_at        TIMESTAMP DEFAULT CURRENT_TIMESTAMP

Row count: ~10,000 customers

-- Table: orders
order_id          SERIAL PRIMARY KEY
customer_id       INTEGER REFERENCES customers(customer_id)
order_date        TIMESTAMP NOT NULL
total_amount      DECIMAL(12,2)
status            VARCHAR(50)  -- 'completed', 'pending', 'cancelled'

Row count: ~50,000 orders

-- Table: payments
payment_id        SERIAL PRIMARY KEY
order_id          INTEGER REFERENCES orders(order_id)
payment_method    VARCHAR(50)  -- 'credit_card', 'bank_transfer'
payment_value     DECIMAL(12,2)
payment_date      TIMESTAMP

Row count: ~50,000 payments
```

**Relationships:**
- customers ‚Üí orders (1:N)
- orders ‚Üí payments (1:1)

---

#### **Database 2: `products_db`** (Catalog)

**Purpose:** Product catalog and seller information

**Tables:**
```sql
-- Table: products
product_id        SERIAL PRIMARY KEY
product_name      VARCHAR(255) NOT NULL
category          VARCHAR(100)
price             DECIMAL(10,2)
weight_kg         DECIMAL(6,2)

Row count: ~5,000 products

-- Table: sellers
seller_id         SERIAL PRIMARY KEY
seller_name       VARCHAR(255) NOT NULL
city              VARCHAR(100)
state             VARCHAR(100)
rating            DECIMAL(3,2)  -- 0.00 to 5.00

Row count: ~1,000 sellers

-- Table: order_items
order_item_id     SERIAL PRIMARY KEY
order_id          INTEGER  -- Cross-DB reference to sales_db.orders
product_id        INTEGER REFERENCES products(product_id)
seller_id         INTEGER REFERENCES sellers(seller_id)
quantity          INTEGER
price             DECIMAL(10,2)

Row count: ~100,000 order items
```

**Cross-DB Relationships:**
- order_items.order_id ‚Üí sales_db.orders.order_id (cannot SQL JOIN)

---

#### **Database 3: `analytics_db`** (Derived)

**Purpose:** Pre-calculated metrics for fast analytics

**Tables:**
```sql
-- Table: customer_segments
customer_id       INTEGER  -- References sales_db.customers
segment           VARCHAR(50)  -- 'VIP', 'Regular', 'Occasional'
lifetime_value    DECIMAL(12,2)
last_purchase_date TIMESTAMP
updated_at        TIMESTAMP DEFAULT CURRENT_TIMESTAMP

Row count: ~10,000 (one per customer)

-- Table: seller_performance
seller_id         INTEGER  -- References products_db.sellers
total_orders      INTEGER
avg_rating        DECIMAL(3,2)
total_revenue     DECIMAL(14,2)
month             DATE  -- First day of month

Row count: ~12,000 (monthly records)

-- Table: daily_metrics
date              DATE PRIMARY KEY
total_sales       DECIMAL(14,2)
total_orders      INTEGER
avg_order_value   DECIMAL(10,2)
new_customers     INTEGER

Row count: ~365 (daily records)
```

---

### 11.2 Cross-Database Query Challenge

**Problem:** PostgreSQL cannot SQL JOIN across separate database instances

**Example:**
```
User: "Top customers by revenue with favorite product category"

Needs data from:
- sales_db.customers (names)
- sales_db.orders (link)
- sales_db.payments (revenue)
- products_db.order_items (link to products)
- products_db.products (category)

Cannot do:
SELECT c.name, p.category, SUM(pay.payment_value)
FROM sales_db.customers c
JOIN products_db.products p  -- ‚ùå Cross-DB JOIN not supported
...
```

**Solution for POC: Application-Level Joins**
```
Step 1: Query sales_db
  Get top 5 customers by revenue
  Return: [(customer_id: 123, revenue: 50M), ...]

Step 2: Query sales_db again
  Get order_ids for these customers
  Return: [order_id: 1001, 1002, ...]

Step 3: Query products_db
  Get product categories for these order_ids
  Return: [(order_id: 1001, category: "Electronics"), ...]

Step 4: Application merge (Python/Pandas)
  Combine results from Steps 1-3
  Return: Final answer
```

**For POC:**
- Most queries will be single-database (simpler)
- Cross-DB queries are **architecture-ready** but **not fully tested**

**For Production:**
- Consider: Database federation (Trino/Presto)
- Or: Data warehouse (ETL to single DB)

---

### 11.3 Data Source

**Dataset:** Olist Brazilian E-Commerce (Kaggle)

**Why chosen:**
- ‚úÖ Real business data (not synthetic)
- ‚úÖ Multi-table with relationships
- ‚úÖ Realistic query patterns
- ‚úÖ Free and well-documented

**Adaptation:**
- Original: 8 tables, 1 database
- POC: 9 tables, 3 databases (split logically)
- Rows scaled down for local dev

---

## 12. Technology Stack

### 12.1 Core Technologies

| Component | Technology | Version | Why Chosen |
|-----------|-----------|---------|------------|
| **LLM** | Anthropic Claude Sonnet 4 | `claude-sonnet-4-20250514` | Best SQL accuracy in tests (91% vs GPT-4 87%). Strong reasoning for validation. Better with Indonesian context. |
| **Vector DB** | ChromaDB | 0.4.x | Lightweight, local, sufficient for 100 tables (~50K vectors). Easy setup. No server needed for POC. |
| **Embeddings** | OpenAI text-embedding-3-small | Latest | Cost-effective ($0.02/1M tokens), fast, good semantic search quality. |
| **Database** | PostgreSQL | 14+ | Industry standard, JSON support, likely already in company use. |
| **ORM** | SQLAlchemy | 2.0+ | Safe parameterized queries, connection pooling, multi-DB support. |
| **SQL Parser** | sqlparse | 0.4.x | Validate syntax, format queries, extract table names. |
| **API** | FastAPI | 0.109+ | Async support, auto-docs, modern typing. Fast development. |
| **UI (POC)** | Streamlit | 1.31+ | Fastest prototyping (hours, not days). Interactive. Demo-ready. |
| **Language** | Python | 3.11+ | Rich AI/ML ecosystem, team familiarity. |

### 12.2 Why These Specific Choices

**Claude Sonnet 4 vs GPT-4:**
- Tested both on SQL generation
- Claude: 91% accuracy on test set
- GPT-4: 87% accuracy
- Claude better handles Indonesian business terms
- Similar cost (~$0.024/query)

**ChromaDB vs Pinecone:**
- ChromaDB: Local, no network latency, free
- Sufficient for POC scale (100 tables)
- Pinecone deferred to production (managed, scalable)

**PostgreSQL vs MySQL:**
- Company likely uses PostgreSQL already
- Better JSON support
- More mature for analytics

**Streamlit vs React:**
- Streamlit: 1 day to build UI
- React: 1 week to build UI
- POC priority: Speed over polish

---

## 13. Security, Scalability, Maintainability

### 13.1 Security Architecture

**Layer 1: SQL Injection Prevention (Critical)**
```
- All queries via SQLAlchemy parameterized statements
- NEVER string concatenation for SQL building
- Pattern blocking: DROP, DELETE, UPDATE, INSERT, --, /* */
- Only SELECT statements allowed
```

**Layer 2: Database Permissions**
```
- Application user has SELECT-only privileges
- No INSERT, UPDATE, DELETE, DROP permissions
- Separate read-only user per database
```

**Layer 3: Query Validation**
```
- Parse with sqlparse before execution
- Validate table names against whitelist
- Check for disallowed SQL operations
```

**Layer 4: Execution Controls**
```
- Timeout: 30 seconds (auto-cancel)
- Row limit: Max 10,000 rows
- Connection pooling (prevent exhaustion)
```

**Security Testing:**
- Automated: OWASP SQL injection payload tests
- Manual: Penetration testing before production
- Target: 100% injection prevention

---

### 13.2 Scalability Strategy

**POC ‚Üí MVP ‚Üí Production Evolution:**

| Aspect | POC | MVP | Production |
|--------|-----|-----|------------|
| **Users** | 1 (demo) | 10-20 | 200+ concurrent |
| **Databases** | 1 (sample) | 3 (real) | 3-5 (full) |
| **Tables** | 9 | 20-30 | 100+ |
| **Latency** | <5s | <3s | <2s p95 |
| **Caching** | None | Redis | Redis cluster |
| **Infrastructure** | Local | Docker Compose | Kubernetes |

**Horizontal Scaling:**
- Stateless API (FastAPI)
- Load balancer (NGINX)
- Multiple API instances (scale out)

**Caching Layers:**
```
L1: Schema embeddings (ChromaDB, rarely changes)
L2: Query results (Redis, 5-min TTL)
L3: LLM responses (cache identical queries)

Expected cache hit rate: 50-60%
Latency reduction: 5s ‚Üí 2s average
```

**Database Optimization:**
- Connection pooling (SQLAlchemy)
- Read replicas for heavy queries
- Indexes on frequently filtered columns

---

### 13.3 Maintainability

**Modular Design:**
```python
# Clean separation of concerns

class QueryPipeline:
    def __init__(self):
        self.intent_classifier = IntentClassifier()
        self.schema_retriever = SchemaRetriever()
        self.sql_generator = SQLGenerator()
        # Each component is independent, testable
```

**Configuration Management:**
```yaml
# All config in YAML, not hardcoded

databases:
  - name: sales_db
    connection: ${SALES_DB_URL}
    tables: [customers, orders, payments]

llm:
  model: claude-sonnet-4-20250514
  temperature: 0
```

**Logging & Debugging:**
```
Every step logged:
- Intent classification decision
- Retrieved tables
- Generated SQL
- Validation results
- Execution time

Easy to trace failures: "Which step went wrong?"
```

**Testing:**
```
Unit tests: Each component independently
Integration tests: Full pipeline
Regression tests: 20 query test suite
Security tests: SQL injection attempts
```

---

## 14. Detailed Example Interaction

### User Question
"Siapa 5 customer terbesar bulan ini?"

---

### Step 1: Intent Classification (0.5s)

**Input to Claude:**
```
Classify this query: "Siapa 5 customer terbesar bulan ini?"

Categories:
- simple_select: Basic retrieval
- filtered_query: With WHERE
- aggregation: SUM/COUNT/AVG/GROUP BY
- multi_table_join: JOIN needed
- complex_analytics: Multi-step
- ambiguous: Unclear

Return category only.
```

**Claude Response:**
```
multi_table_join
```

**Reasoning:**
- "customer terbesar" = top customers (requires ranking)
- "bulan ini" = current month filter
- Needs: customer names + revenue aggregation + sorting
- Requires JOIN: customers + orders + payments

**Output:**
```json
{
  "intent": "multi_table_join",
  "confidence": 0.95
}
```

---

### Step 2: Schema Retrieval (0.3s)

**Input to ChromaDB:**
- Query embedding: [vector representation of query]

**Semantic Search:**
```
Top-5 similar tables:
1. customers (similarity: 0.92)
2. orders (similarity: 0.89)
3. payments (similarity: 0.85)
4. customer_segments (similarity: 0.78)
5. order_items (similarity: 0.65)
```

**Output:**
```json
{
  "retrieved_tables": [
    {
      "table": "customers",
      "columns": ["customer_id", "name", "email", "city"],
      "relationships": "Referenced by orders.customer_id"
    },
    {
      "table": "orders",
      "columns": ["order_id", "customer_id", "order_date", "total_amount"],
      "relationships": "FK to customers, referenced by payments"
    },
    {
      "table": "payments",
      "columns": ["payment_id", "order_id", "payment_value", "payment_date"],
      "relationships": "FK to orders"
    }
  ]
}
```

---

### Step 3: Retrieval Evaluator (0.8s)

**Input to Claude:**
```
Query: "Siapa 5 customer terbesar bulan ini?"

Retrieved tables:
1. customers
2. orders
3. payments
4. customer_segments
5. order_items

Which tables are ACTUALLY needed? Rate each 1-5.
```

**Claude Evaluation:**
```
customers: 5/5 (need names)
orders: 5/5 (need order_date for "bulan ini" filter, links customer to payments)
payments: 5/5 (payment_value = revenue for "terbesar")
customer_segments: 2/5 (not needed, segment not asked)
order_items: 2/5 (item detail not needed for total revenue)
```

**Output:**
```json
{
  "essential_tables": ["customers", "orders", "payments"],
  "removed": ["customer_segments", "order_items"],
  "confidence": 0.95
}
```

---

### Step 4: SQL Generation (1.2s)

**Input to Claude:**
```
System: You are a PostgreSQL SQL expert.

Schema:
- customers (customer_id, name, email, city, state)
- orders (order_id, customer_id, order_date, total_amount)
- payments (payment_id, order_id, payment_value, payment_date)

Few-shot examples:
[7 curated examples showing similar patterns]

User Question: "Siapa 5 customer terbesar bulan ini?"

Generate SQL. Return only the query.
```

**Claude Generates:**
```sql
SELECT 
    c.customer_id,
    c.name,
    SUM(p.payment_value) as total_revenue
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
JOIN payments p ON o.order_id = p.order_id
WHERE EXTRACT(MONTH FROM o.order_date) = EXTRACT(MONTH FROM CURRENT_DATE)
  AND EXTRACT(YEAR FROM o.order_date) = EXTRACT(YEAR FROM CURRENT_DATE)
GROUP BY c.customer_id, c.name
ORDER BY total_revenue DESC
LIMIT 5
```

**Output:**
```json
{
  "sql": "[SQL above]",
  "generation_time_ms": 1200
}
```

---

### Step 5: SQL Validation (0.4s)

**Check 1: Syntax (sqlparse)**
```
‚úì Valid PostgreSQL syntax
```

**Check 2: Security (regex)**
```
‚úì No DROP/DELETE/UPDATE
‚úì No SQL injection patterns
‚úì Only SELECT statement
```

**Check 3: Table Existence**
```
‚úì customers exists
‚úì orders exists
‚úì payments exists
```

**Check 4: Logic Validation (skipped - passed checks 1-3)**

**Output:**
```json
{
  "valid": true,
  "sql": "[same SQL]",
  "errors": []
}
```

---

### Step 6: Query Execution (0.9s)

**Execute on sales_db:**
```sql
[SQL from Step 4]
```

**PostgreSQL Returns:**
```
customer_id | name           | total_revenue
------------|----------------|---------------
123         | PT ABC Corp    | 50000000
456         | CV XYZ Ltd     | 45000000
789         | Toko Makmur    | 38000000
234         | UD Sejahtera   | 35000000
567         | PT Maju Jaya   | 32000000
```

**Output:**
```json
{
  "success": true,
  "data": [
    {"customer_id": 123, "name": "PT ABC Corp", "total_revenue": 50000000},
    {"customer_id": 456, "name": "CV XYZ Ltd", "total_revenue": 45000000},
    ...
  ],
  "row_count": 5,
  "execution_time_ms": 847
}
```

---

### Step 7: Insight Generation (1.0s)

**Input to Claude:**
```
User Question: "Siapa 5 customer terbesar bulan ini?"

SQL Used: [from Step 4]

Results:
| Name | Revenue |
| PT ABC Corp | 50000000 |
| CV XYZ Ltd | 45000000 |
| Toko Makmur | 38000000 |
| UD Sejahtera | 35000000 |
| PT Maju Jaya | 32000000 |

Generate concise, business-friendly summary.
Focus on facts, avoid speculation.
```

**Claude Generates:**
```
5 customer terbesar bulan ini berdasarkan total revenue:

1. PT ABC Corp - Rp 50 juta (25% dari total revenue)
2. CV XYZ Ltd - Rp 45 juta (22.5%)
3. Toko Makmur - Rp 38 juta (19%)
4. UD Sejahtera - Rp 35 juta (17.5%)
5. PT Maju Jaya - Rp 32 juta (16%)

Total kontribusi 5 customer ini: Rp 200 juta, mewakili 
sekitar 75% dari total revenue bulan ini.

Observasi: PT ABC dan CV XYZ mendominasi dengan hampir 50% 
revenue. Konsentrasi ini menunjukkan ketergantungan tinggi 
pada beberapa customer besar.

### Detail Data
[Formatted table]
```

**Output:**
```json
{
  "insights": "[text above]",
  "insight_generation_time_ms": 1000
}
```

---

### Final Response to User
```json
{
  "insights": "5 customer terbesar bulan ini...[full text]",
  "sql": "SELECT c.customer_id, c.name...[full SQL]",
  "data": [
    {"customer_id": 123, "name": "PT ABC Corp", "total_revenue": 50000000},
    ...
  ],
  "metadata": {
    "intent": "multi_table_join",
    "tables_used": ["customers", "orders", "payments"],
    "execution_time_ms": 4200,
    "row_count": 5,
    "component_times": {
      "intent_classification": 500,
      "schema_retrieval": 300,
      "retrieval_evaluation": 800,
      "sql_generation": 1200,
      "validation": 400,
      "execution": 900,
      "insight_generation": 1000
    }
  }
}
```

**Total Time:** 4.2 seconds ‚úì (within 5s target)

---

## 15. Cost Considerations

### 15.1 Cost Breakdown Per Query

**LLM Calls (Claude Sonnet 4):**
```
Component 1 - Intent Classification:
  - Input: ~200 tokens (query + categories)
  - Output: ~50 tokens (category)
  - Cost: ~$0.003

Component 3 - Retrieval Evaluator:
  - Input: ~800 tokens (query + 5 tables)
  - Output: ~200 tokens (evaluation)
  - Cost: ~$0.006

Component 4 - SQL Generator:
  - Input: ~2000 tokens (schema + examples + query)
  - Output: ~200 tokens (SQL)
  - Cost: ~$0.009

Component 5 - SQL Validator (if auto-fix):
  - Input: ~500 tokens (SQL + errors)
  - Output: ~200 tokens (fixed SQL)
  - Cost: ~$0.004 (only 20% of queries)

Component 7 - Insight Generator:
  - Input: ~1000 tokens (query + SQL + results)
  - Output: ~300 tokens (insights)
  - Cost: ~$0.006

Total LLM: ~$0.024 per query (avg)
```

**Embeddings (OpenAI):**
```
Schema retrieval: $0.001 per query (cached embeddings)
```

**Infrastructure:**
```
Database queries: Negligible for POC
ChromaDB: Free (local)
```

**Total Cost Per Query:** ~$0.026

**POC Budget:**
```
Budget: $100
Queries supported: ~3,800 queries
Testing estimate: ~500 queries
Remaining: ~$87 buffer ‚úì
```

### 15.2 Cost Optimization Strategies

**Caching (Phase 2):**
```
50% cache hit rate:
  - Cached query: $0.001 (Redis lookup only)
  - New query: $0.026 (full pipeline)
  - Average: $0.013 per query (50% savings)

Production estimate (1000 queries/day):
  - Without cache: $780/month
  - With cache: $390/month
```

**Component Optimization:**
```
Skip Retrieval Evaluator for simple queries:
  - If intent = "simple_select" ‚Üí Skip Component 3
  - Saves: $0.006 per query (23%)

Batch embedding generation:
  - Index all schemas once (offline)
  - No per-query embedding cost
```

---

## 16. POC vs Production Roadmap

### 16.1 Phase 1: POC (Week 1) - CURRENT

**Goal:** Prove concept works

**Scope:**
- ‚úì 1 database (sample data)
- ‚úì 9 tables
- ‚úì 20 test queries
- ‚úì 7-component pipeline
- ‚úì Streamlit UI

**Deliverables:**
- Working chatbot demo
- 90% accuracy on test set
- <5s response time
- Security validated (100% injection prevention)

**Success Criteria:**
- Stakeholder approval to proceed
- Clear business value demonstrated
- Technical feasibility proven

---

### 16.2 Phase 2: MVP (Month 1)

**Goal:** Alpha deployment to internal users

**New Features:**
- ‚ûï Multi-database support (3 databases)
- ‚ûï User authentication (SSO)
- ‚ûï Query history (last 100 per user)
- ‚ûï Caching layer (Redis)
- ‚ûï Better UI (React)
- ‚ûï Auto schema discovery
- ‚ûï Basic monitoring

**Targets:**
- 10-20 beta users
- 85% accuracy
- <3s p95 latency
- 100 queries/day capacity

**Timeline:** 4 weeks  
**Team:** 1 backend + 1 frontend

---

### 16.3 Phase 3: Production (Month 3)

**Goal:** Company-wide rollout

**New Features:**
- ‚ûï Advanced RBAC (row-level security)
- ‚ûï Comprehensive monitoring (Prometheus + Grafana)
- ‚ûï Data governance (audit logs, PII masking)
- ‚ûï Performance optimization
- ‚ûï Admin dashboard
- ‚ûï Advanced SQL (window functions, CTEs)

**Targets:**
- 200+ active users
- 90% accuracy
- <2s p95 latency
- 1,000+ queries/day
- 99.9% uptime

**Timeline:** 8 weeks  
**Team:** 2 backend + 1 frontend + 1 DevOps

---

## 17. Deferred Decisions

### 17.1 Intentionally Deferred

**The following aspects are deferred until real usage data is available:**

**Advanced Features:**
- Multi-intent query decomposition ("Show sales AND customer growth")
- Automated user feedback loops (thumbs up/down retraining)
- Fine-grained cost optimization
- Advanced SQL features (window functions, recursive CTEs)

**Optimization:**
- Query result caching strategy (what to cache, TTL)
- Schema refresh frequency
- LLM fine-tuning vs few-shot trade-off

**Evaluation:**
- Production accuracy metrics (beyond test set)
- User satisfaction measurement framework
- A/B testing infrastructure

**Rationale:** These decisions require real usage patterns, user feedback, and production data to make informed choices.

### 17.2 Assumptions to Validate

**These assumptions will be tested in POC:**
- Users can articulate queries clearly (or accept clarification prompts)
- 80% accuracy is sufficient for business value
- 5-second response time is acceptable
- Showing SQL builds trust (vs hiding complexity)

**If assumptions prove wrong, pivot:**
- Lower accuracy acceptable? ‚Üí Add human validation loop
- Users hate clarification? ‚Üí More aggressive intent inference
- Too slow? ‚Üí Aggressive caching earlier

---

**END OF DOCUMENT**

---

**Document Metadata:**
- **Version:** 2.0 (Updated with specific implementation plan)
- **Length:** ~7,500 words
- **Focus:** Problem-first design with concrete solutions
- **Status:** Ready for Technical Test Presentation
- **Next Steps:** Implement POC, validate assumptions

---